{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oglzm-Z3aif"
      },
      "source": [
        "### **Section 1: Data Prep**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJRtXZRRcJb2",
        "outputId": "00ebe493-bb37-4380-d1fe-358ef1df269c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting m2cgen\n",
            "  Downloading m2cgen-0.9.0-py3-none-any.whl (73 kB)\n",
            "Requirement already satisfied: numpy in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from m2cgen) (1.19.5)\n",
            "Installing collected packages: m2cgen\n",
            "Successfully installed m2cgen-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install m2cgen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6AAmwSO6wpR"
      },
      "source": [
        "#### **1.1 Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzMdhEEt0dRR",
        "outputId": "247aaddd-d89c-4d0c-d0c1-7f72a36830d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Collecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-win_amd64.whl (6.8 MB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from scikit-learn==0.24.2) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from scikit-learn==0.24.2) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from scikit-learn==0.24.2) (1.6.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from scikit-learn==0.24.2) (2.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "Successfully installed scikit-learn-1.0\n"
          ]
        }
      ],
      "source": [
        "# Install the Updated Version of SKLearn if Nedded\n",
        "!pip install scikit-learn==0.24.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "g-fRgFfnS1NR",
        "outputId": "c7f9ceea-5d09-4dd1-dd7f-1447a20a40c5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use(\"ggplot\")\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from PIL import Image as im\n",
        "import sys\n",
        "sys.setrecursionlimit(2147483647)\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import pandas as pd\n",
        "style.use(\"ggplot\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load Training Data and Evaluation Data.\n",
        "def data_prep_multiclass(path_prefix, class_idx):\n",
        "    results = []\n",
        "    for name in [\"eval\", \"train\"]:\n",
        "        csv = pd.read_csv(f\"{path_prefix}{name}.csv\")\n",
        "        csv = csv.iloc[:, 0:258]\n",
        "        csv.dropna(inplace=True)\n",
        "        csv.drop_duplicates(inplace=True)\n",
        "        csv.drop(csv[(csv['label'] != 0) & (csv['label'] != class_idx)].index, inplace = True)\n",
        "\n",
        "        csv.reset_index(drop=True,inplace=True)\n",
        "        \n",
        "        for i in range(len(csv)):\n",
        "            if csv['label'][i] != 0: csv['label'][i] = 1\n",
        "    \n",
        "        results.append(csv)\n",
        "    return results\n",
        "\n",
        "def data_prep_binary(path_prefix):\n",
        "    results = []\n",
        "    for name in [\"eval\", \"train\"]:\n",
        "        csv = pd.read_csv(f\"{path_prefix}{name}.csv\")\n",
        "        csv = csv.iloc[:, 0:258]\n",
        "        csv.dropna(inplace=True)\n",
        "        csv.drop_duplicates(inplace=True)\n",
        "        csv.reset_index(drop=True,inplace=True)\n",
        "        \n",
        "        for i in range(len(csv)):\n",
        "            if csv['label'][i] != 0: csv['label'][i] = 1\n",
        "    \n",
        "        results.append(csv)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o2LQ6xa5lyHl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3565,)\n",
            "(3565, 1, 16, 16)\n",
            "(35166,)\n",
            "(35166, 1, 16, 16)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0., 1.])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_csv, eval_csv = data_prep_binary(\"data/1126\")\n",
        "\n",
        "class PandasDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.transform = transform\n",
        "        self.dataframe = dataframe\n",
        "        self.labels = self.dataframe.label.to_numpy().astype(float)\n",
        "        self.data = self.dataframe.iloc[:, 1:257].to_numpy()\n",
        "        self.data = (self.data - self.data.mean(1, keepdims=True)) / (self.data.std(1, keepdims=True) + 1e-8)\n",
        "        self.data = self.data.reshape(-1, 1, 16, 16).astype(float)\n",
        "        print(self.labels.shape)\n",
        "        print(self.data.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.data[index, ...]\n",
        "        if self.transform:\n",
        "            img = self.transform(Image.fromarray(img[0, ...]))\n",
        "        target = float(self.labels[index])\n",
        "        return img, target\n",
        "\n",
        "batch_size_train = 1280\n",
        "batch_size_test = 1280\n",
        "total_size  = len(train_csv)\n",
        "train_size = int(total_size * 0.8)\n",
        "test_size = total_size - train_size\n",
        "# train_dataset, test_dataset = random_split(PandasDataset(train_csv, transform=transforms),\n",
        "train_dataset, test_dataset = random_split(PandasDataset(train_csv, transform=None),\n",
        "                                           [train_size, test_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=True)\n",
        "eval_loader = torch.utils.data.DataLoader(PandasDataset(eval_csv,transform=None), \n",
        "                                          batch_size=len(eval_csv), shuffle=False)\n",
        "train_csv.label.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "0YiajcKjkYpN",
        "outputId": "9fe2e1a9-4ffa-4915-fe94-692aa0b26c93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1280, 1, 16, 16])\n",
            "torch.Size([1280])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "examples = enumerate(train_loader)\n",
        "idx, example_data = next(examples)\n",
        "imgs, labels = example_data\n",
        "print(imgs.shape)\n",
        "print(labels.shape)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(imgs[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(labels[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPsy2lAonH4_"
      },
      "source": [
        "#### **2.2 NN Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwoDCNN(nn.Module):\n",
        "  def __init__(self, config=[1, 4, 8, 16, 32], dropout=0.5):\n",
        "    super(TwoDCNN, self).__init__()\n",
        "    \n",
        "    modules = []\n",
        "    for i, dim in enumerate(config[1:-1]):\n",
        "      modules += [\n",
        "        nn.Conv2d(config[i], dim, 3, padding=1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.BatchNorm2d(dim),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.MaxPool2d(2)\n",
        "      ]\n",
        "    modules += [\n",
        "      nn.Conv2d(config[-2], config[-1], 3, padding=1),\n",
        "      # nn.LeakyReLU(),\n",
        "      nn.BatchNorm2d(config[-1]),\n",
        "      nn.AdaptiveAvgPool2d((1, 1)),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(config[-1], 1),\n",
        "      nn.Sigmoid()\n",
        "    ]\n",
        "    self.net = nn.Sequential(*modules)\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.net:\n",
        "        x = layer(x)\n",
        "    return x.reshape(-1)\n",
        "\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    \n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h = self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b p h n d -> b p n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads, dim_head, dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
        "            ]))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class MV2Block(nn.Module):\n",
        "    def __init__(self, inp, oup, stride=1, expansion=4):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = int(inp * expansion)\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        if expansion == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.SiLU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileViTBlock(nn.Module):\n",
        "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.ph, self.pw = patch_size\n",
        "\n",
        "        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
        "        self.conv2 = conv_1x1_bn(channel, dim)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, 1, 32, mlp_dim, dropout)\n",
        "\n",
        "        self.conv3 = conv_1x1_bn(dim, channel)\n",
        "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        y = x.clone()\n",
        "\n",
        "        # Local representations\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        # Global representations\n",
        "        _, _, h, w = x.shape\n",
        "        x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)\n",
        "        x = self.transformer(x)\n",
        "        x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h//self.ph, w=w//self.pw, ph=self.ph, pw=self.pw)\n",
        "\n",
        "        # Fusion\n",
        "        x = self.conv3(x)\n",
        "        x = torch.cat((x, y), 1)\n",
        "        x = self.conv4(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# our mobilevit\n",
        "class TFTwoDCNN(nn.Module):\n",
        "  def __init__(self, config=[1, 4, 8, 16, 16], maxpool=[True, True, True], dropout=0.5):\n",
        "    super(TFTwoDCNN, self).__init__()\n",
        "    \n",
        "    modules = []\n",
        "    for i, dim in enumerate(config[1:-1]):\n",
        "      modules += [\n",
        "        nn.Conv2d(config[i], dim, 3, padding=1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.BatchNorm2d(dim),\n",
        "        nn.Dropout(dropout)\n",
        "      ]\n",
        "      if maxpool[i]:\n",
        "        modules += [nn.MaxPool2d(2)]\n",
        "      modules += [MobileViTBlock(dim, 2, dim, 3, (2, 2), dim*3)]\n",
        "    modules += [\n",
        "      nn.Conv2d(config[-2], config[-1], 3, padding=1),\n",
        "      nn.BatchNorm2d(config[-1]),\n",
        "      nn.AdaptiveAvgPool2d((1, 1)),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(config[-1], 1),\n",
        "      nn.Sigmoid()\n",
        "    ]\n",
        "    self.net = nn.Sequential(*modules)\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.net:\n",
        "        x = layer(x)\n",
        "    return x.reshape(-1)\n",
        "\n",
        "criterion = nn.BCELoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7E1xhwyCK1ia",
        "outputId": "4bdbad55-c6e3-4230-8e58-b908de73d77c"
      },
      "outputs": [],
      "source": [
        "def test_eval(model, test_loader, eval_loader):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model.eval()\n",
        "  acc = {}\n",
        "  with torch.no_grad():\n",
        "    # for name, loader in [(\"test\", test_loader), (\"eval\", eval_loader)]:\n",
        "    for name, loader in [(\"test\", test_loader)]:\n",
        "      correct_count, all_count = 0, 0\n",
        "      for images, labels in loader:\n",
        "        images = images.type(torch.float).to(device)\n",
        "        labels = labels.type(torch.float).to(device)\n",
        "        out = model(images)\n",
        "        pred = out > 0.5\n",
        "        correct_count += (labels == pred).sum().item()\n",
        "        all_count += images.shape[0]\n",
        "      acc[name] = correct_count / all_count\n",
        "      print(f\"On %s set: Acc %.3f\" % (\n",
        "        name, acc[name]))\n",
        "  # return acc[\"test\"], acc[\"eval\"]\n",
        "  return acc[\"test\"], 0\n",
        "\n",
        "def train(model, train_loader, test_loader, eval_loader, epochs, lr):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = model.to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "  eval_acc_list = []\n",
        "  test_acc_list = []\n",
        "  train_acc_list = []\n",
        "  for e in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    ct, all_count = 0, 0\n",
        "    for images, labels in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      images = images.type(torch.float).to(device)\n",
        "      labels = labels.type(torch.float).to(device)\n",
        "      output = model(images)\n",
        "      loss = criterion(output, labels)\n",
        "      pred = output > 0.5\n",
        "      ct += (labels == pred).sum().item()\n",
        "      all_count += images.shape[0]\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      ct += 1\n",
        "      running_loss += loss.item()\n",
        "    train_acc_list.append(ct / all_count)\n",
        "    print(\"Epoch {} - Training loss: {:.3f}\".format(e+1, running_loss/len(train_loader)))\n",
        "    print(f\"On train set: Acc %.3f\" % (ct / all_count))\n",
        "    test_acc, eval_acc = test_eval(model, test_loader, eval_loader)\n",
        "    eval_acc_list.append(test_acc)\n",
        "    test_acc_list.append(eval_acc)\n",
        "  print(f\"Best Train Acc: {max(train_acc_list)}\")\n",
        "  print(f\"Best Test Acc: {max(eval_acc_list)}\")\n",
        "  # print(f\"Best Eval Acc: {max(test_acc_list)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Training loss: 0.721\n",
            "On train set: Acc 0.519\n",
            "On test set: Acc 0.750\n",
            "Epoch 2 - Training loss: 0.476\n",
            "On train set: Acc 0.859\n",
            "On test set: Acc 0.875\n",
            "Epoch 3 - Training loss: 0.329\n",
            "On train set: Acc 0.860\n",
            "On test set: Acc 0.896\n",
            "Epoch 4 - Training loss: 0.253\n",
            "On train set: Acc 0.899\n",
            "On test set: Acc 0.886\n",
            "Epoch 5 - Training loss: 0.218\n",
            "On train set: Acc 0.907\n",
            "On test set: Acc 0.907\n",
            "Epoch 6 - Training loss: 0.196\n",
            "On train set: Acc 0.928\n",
            "On test set: Acc 0.895\n",
            "Epoch 7 - Training loss: 0.175\n",
            "On train set: Acc 0.930\n",
            "On test set: Acc 0.912\n",
            "Epoch 8 - Training loss: 0.157\n",
            "On train set: Acc 0.940\n",
            "On test set: Acc 0.912\n",
            "Epoch 9 - Training loss: 0.147\n",
            "On train set: Acc 0.946\n",
            "On test set: Acc 0.916\n",
            "Epoch 10 - Training loss: 0.130\n",
            "On train set: Acc 0.954\n",
            "On test set: Acc 0.917\n",
            "Epoch 11 - Training loss: 0.098\n",
            "On train set: Acc 0.959\n",
            "On test set: Acc 0.896\n",
            "Epoch 12 - Training loss: 0.109\n",
            "On train set: Acc 0.971\n",
            "On test set: Acc 0.916\n",
            "Epoch 13 - Training loss: 0.118\n",
            "On train set: Acc 0.971\n",
            "On test set: Acc 0.941\n",
            "Epoch 14 - Training loss: 0.095\n",
            "On train set: Acc 0.965\n",
            "On test set: Acc 0.961\n",
            "Epoch 15 - Training loss: 0.082\n",
            "On train set: Acc 0.973\n",
            "On test set: Acc 0.971\n",
            "Epoch 16 - Training loss: 0.081\n",
            "On train set: Acc 0.972\n",
            "On test set: Acc 0.965\n",
            "Epoch 17 - Training loss: 0.079\n",
            "On train set: Acc 0.976\n",
            "On test set: Acc 0.962\n",
            "Epoch 18 - Training loss: 0.077\n",
            "On train set: Acc 0.976\n",
            "On test set: Acc 0.976\n",
            "Epoch 19 - Training loss: 0.070\n",
            "On train set: Acc 0.978\n",
            "On test set: Acc 0.955\n",
            "Epoch 20 - Training loss: 0.064\n",
            "On train set: Acc 0.979\n",
            "On test set: Acc 0.923\n",
            "Epoch 21 - Training loss: 0.067\n",
            "On train set: Acc 0.975\n",
            "On test set: Acc 0.978\n",
            "Epoch 22 - Training loss: 0.060\n",
            "On train set: Acc 0.980\n",
            "On test set: Acc 0.928\n",
            "Epoch 23 - Training loss: 0.080\n",
            "On train set: Acc 0.977\n",
            "On test set: Acc 0.959\n",
            "Epoch 24 - Training loss: 0.069\n",
            "On train set: Acc 0.980\n",
            "On test set: Acc 0.971\n",
            "Epoch 25 - Training loss: 0.068\n",
            "On train set: Acc 0.977\n",
            "On test set: Acc 0.944\n",
            "Epoch 26 - Training loss: 0.071\n",
            "On train set: Acc 0.974\n",
            "On test set: Acc 0.877\n",
            "Epoch 27 - Training loss: 0.062\n",
            "On train set: Acc 0.977\n",
            "On test set: Acc 0.965\n",
            "Epoch 28 - Training loss: 0.049\n",
            "On train set: Acc 0.981\n",
            "On test set: Acc 0.965\n",
            "Epoch 29 - Training loss: 0.056\n",
            "On train set: Acc 0.978\n",
            "On test set: Acc 0.959\n",
            "Epoch 30 - Training loss: 0.059\n",
            "On train set: Acc 0.982\n",
            "On test set: Acc 0.982\n",
            "Epoch 31 - Training loss: 0.061\n",
            "On train set: Acc 0.985\n",
            "On test set: Acc 0.947\n",
            "Epoch 32 - Training loss: 0.051\n",
            "On train set: Acc 0.987\n",
            "On test set: Acc 0.948\n",
            "Epoch 33 - Training loss: 0.073\n",
            "On train set: Acc 0.986\n",
            "On test set: Acc 0.982\n",
            "Epoch 34 - Training loss: 0.055\n",
            "On train set: Acc 0.985\n",
            "On test set: Acc 0.889\n",
            "Epoch 35 - Training loss: 0.071\n",
            "On train set: Acc 0.975\n",
            "On test set: Acc 0.930\n",
            "Epoch 36 - Training loss: 0.064\n",
            "On train set: Acc 0.979\n",
            "On test set: Acc 0.969\n",
            "Epoch 37 - Training loss: 0.074\n",
            "On train set: Acc 0.979\n",
            "On test set: Acc 0.958\n",
            "Epoch 38 - Training loss: 0.047\n",
            "On train set: Acc 0.987\n",
            "On test set: Acc 0.916\n",
            "Epoch 39 - Training loss: 0.053\n",
            "On train set: Acc 0.978\n",
            "On test set: Acc 0.927\n",
            "Epoch 40 - Training loss: 0.062\n",
            "On train set: Acc 0.985\n",
            "On test set: Acc 0.938\n",
            "Epoch 41 - Training loss: 0.045\n",
            "On train set: Acc 0.984\n",
            "On test set: Acc 0.931\n",
            "Epoch 42 - Training loss: 0.056\n",
            "On train set: Acc 0.980\n",
            "On test set: Acc 0.920\n",
            "Epoch 43 - Training loss: 0.048\n",
            "On train set: Acc 0.985\n",
            "On test set: Acc 0.927\n",
            "Epoch 44 - Training loss: 0.045\n",
            "On train set: Acc 0.986\n",
            "On test set: Acc 0.982\n",
            "Epoch 45 - Training loss: 0.037\n",
            "On train set: Acc 0.991\n",
            "On test set: Acc 0.978\n",
            "Epoch 46 - Training loss: 0.033\n",
            "On train set: Acc 0.990\n",
            "On test set: Acc 0.985\n",
            "Epoch 47 - Training loss: 0.036\n",
            "On train set: Acc 0.991\n",
            "On test set: Acc 0.982\n",
            "Epoch 48 - Training loss: 0.041\n",
            "On train set: Acc 0.989\n",
            "On test set: Acc 0.985\n",
            "Epoch 49 - Training loss: 0.047\n",
            "On train set: Acc 0.988\n",
            "On test set: Acc 0.966\n",
            "Epoch 50 - Training loss: 0.031\n",
            "On train set: Acc 0.992\n",
            "On test set: Acc 0.969\n",
            "Epoch 51 - Training loss: 0.034\n",
            "On train set: Acc 0.993\n",
            "On test set: Acc 0.973\n",
            "Epoch 52 - Training loss: 0.031\n",
            "On train set: Acc 0.992\n",
            "On test set: Acc 0.969\n",
            "Epoch 53 - Training loss: 0.040\n",
            "On train set: Acc 0.991\n",
            "On test set: Acc 0.975\n",
            "Epoch 54 - Training loss: 0.033\n",
            "On train set: Acc 0.988\n",
            "On test set: Acc 0.942\n",
            "Epoch 55 - Training loss: 0.046\n",
            "On train set: Acc 0.988\n",
            "On test set: Acc 0.959\n",
            "Epoch 56 - Training loss: 0.048\n",
            "On train set: Acc 0.982\n",
            "On test set: Acc 0.944\n",
            "Epoch 57 - Training loss: 0.059\n",
            "On train set: Acc 0.982\n",
            "On test set: Acc 0.926\n",
            "Epoch 58 - Training loss: 0.039\n",
            "On train set: Acc 0.987\n",
            "On test set: Acc 0.900\n",
            "Epoch 59 - Training loss: 0.044\n",
            "On train set: Acc 0.986\n",
            "On test set: Acc 0.975\n",
            "Epoch 60 - Training loss: 0.032\n",
            "On train set: Acc 0.991\n",
            "On test set: Acc 0.966\n",
            "Epoch 61 - Training loss: 0.043\n",
            "On train set: Acc 0.988\n",
            "On test set: Acc 0.982\n",
            "Epoch 62 - Training loss: 0.040\n",
            "On train set: Acc 0.990\n",
            "On test set: Acc 0.955\n",
            "Epoch 63 - Training loss: 0.035\n",
            "On train set: Acc 0.990\n",
            "On test set: Acc 0.985\n",
            "Epoch 64 - Training loss: 0.037\n",
            "On train set: Acc 0.987\n",
            "On test set: Acc 0.979\n",
            "Epoch 65 - Training loss: 0.045\n",
            "On train set: Acc 0.987\n",
            "On test set: Acc 0.985\n",
            "Epoch 66 - Training loss: 0.038\n",
            "On train set: Acc 0.991\n",
            "On test set: Acc 0.975\n",
            "Epoch 67 - Training loss: 0.031\n",
            "On train set: Acc 0.989\n",
            "On test set: Acc 0.961\n",
            "Epoch 68 - Training loss: 0.033\n",
            "On train set: Acc 0.991\n",
            "On test set: Acc 0.983\n",
            "Epoch 69 - Training loss: 0.036\n",
            "On train set: Acc 0.992\n",
            "On test set: Acc 0.986\n",
            "Epoch 70 - Training loss: 0.032\n",
            "On train set: Acc 0.991\n",
            "On test set: Acc 0.983\n",
            "Epoch 71 - Training loss: 0.029\n",
            "On train set: Acc 0.993\n",
            "On test set: Acc 0.975\n",
            "Epoch 72 - Training loss: 0.023\n",
            "On train set: Acc 0.993\n",
            "On test set: Acc 0.985\n",
            "Epoch 73 - Training loss: 0.026\n",
            "On train set: Acc 0.994\n",
            "On test set: Acc 0.983\n",
            "Epoch 74 - Training loss: 0.032\n",
            "On train set: Acc 0.991\n",
            "On test set: Acc 0.986\n",
            "Epoch 75 - Training loss: 0.023\n",
            "On train set: Acc 0.996\n",
            "On test set: Acc 0.983\n",
            "Epoch 76 - Training loss: 0.030\n",
            "On train set: Acc 0.993\n",
            "On test set: Acc 0.985\n",
            "Epoch 77 - Training loss: 0.035\n",
            "On train set: Acc 0.989\n",
            "On test set: Acc 0.980\n",
            "Epoch 78 - Training loss: 0.037\n",
            "On train set: Acc 0.987\n",
            "On test set: Acc 0.976\n",
            "Epoch 79 - Training loss: 0.054\n",
            "On train set: Acc 0.979\n",
            "On test set: Acc 0.986\n",
            "Epoch 80 - Training loss: 0.042\n",
            "On train set: Acc 0.991\n",
            "On test set: Acc 0.961\n",
            "Best Train Acc: 0.9957924263674615\n",
            "Best Test Acc: 0.9859747545582047\n"
          ]
        }
      ],
      "source": [
        "model = TwoDCNN(dropout=0.01)\n",
        "train(model, train_loader, test_loader, eval_loader, 80, 0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Training loss: 0.749\n",
            "On train set: Acc 0.492\n",
            "On test set: Acc 0.760\n",
            "Epoch 2 - Training loss: 0.579\n",
            "On train set: Acc 0.690\n",
            "On test set: Acc 0.760\n",
            "Epoch 3 - Training loss: 0.517\n",
            "On train set: Acc 0.748\n",
            "On test set: Acc 0.760\n",
            "Epoch 4 - Training loss: 0.432\n",
            "On train set: Acc 0.777\n",
            "On test set: Acc 0.760\n",
            "Epoch 5 - Training loss: 0.414\n",
            "On train set: Acc 0.789\n",
            "On test set: Acc 0.760\n",
            "Epoch 6 - Training loss: 0.324\n",
            "On train set: Acc 0.837\n",
            "On test set: Acc 0.760\n",
            "Epoch 7 - Training loss: 0.335\n",
            "On train set: Acc 0.842\n",
            "On test set: Acc 0.760\n",
            "Epoch 8 - Training loss: 0.281\n",
            "On train set: Acc 0.867\n",
            "On test set: Acc 0.760\n",
            "Epoch 9 - Training loss: 0.253\n",
            "On train set: Acc 0.885\n",
            "On test set: Acc 0.760\n",
            "Epoch 10 - Training loss: 0.227\n",
            "On train set: Acc 0.913\n",
            "On test set: Acc 0.759\n",
            "Epoch 11 - Training loss: 0.203\n",
            "On train set: Acc 0.917\n",
            "On test set: Acc 0.742\n",
            "Epoch 12 - Training loss: 0.197\n",
            "On train set: Acc 0.917\n",
            "On test set: Acc 0.750\n",
            "Epoch 13 - Training loss: 0.208\n",
            "On train set: Acc 0.920\n",
            "On test set: Acc 0.799\n",
            "Epoch 14 - Training loss: 0.175\n",
            "On train set: Acc 0.931\n",
            "On test set: Acc 0.763\n",
            "Epoch 15 - Training loss: 0.177\n",
            "On train set: Acc 0.935\n",
            "On test set: Acc 0.725\n",
            "Epoch 16 - Training loss: 0.202\n",
            "On train set: Acc 0.920\n",
            "On test set: Acc 0.697\n",
            "Epoch 17 - Training loss: 0.185\n",
            "On train set: Acc 0.933\n",
            "On test set: Acc 0.760\n",
            "Epoch 18 - Training loss: 0.150\n",
            "On train set: Acc 0.938\n",
            "On test set: Acc 0.729\n",
            "Epoch 19 - Training loss: 0.135\n",
            "On train set: Acc 0.951\n",
            "On test set: Acc 0.727\n",
            "Epoch 20 - Training loss: 0.128\n",
            "On train set: Acc 0.947\n",
            "On test set: Acc 0.829\n",
            "Epoch 21 - Training loss: 0.126\n",
            "On train set: Acc 0.942\n",
            "On test set: Acc 0.854\n",
            "Epoch 22 - Training loss: 0.115\n",
            "On train set: Acc 0.955\n",
            "On test set: Acc 0.930\n",
            "Epoch 23 - Training loss: 0.175\n",
            "On train set: Acc 0.944\n",
            "On test set: Acc 0.934\n",
            "Epoch 24 - Training loss: 0.179\n",
            "On train set: Acc 0.917\n",
            "On test set: Acc 0.813\n",
            "Epoch 25 - Training loss: 0.142\n",
            "On train set: Acc 0.945\n",
            "On test set: Acc 0.773\n",
            "Epoch 26 - Training loss: 0.147\n",
            "On train set: Acc 0.943\n",
            "On test set: Acc 0.926\n",
            "Epoch 27 - Training loss: 0.136\n",
            "On train set: Acc 0.944\n",
            "On test set: Acc 0.937\n",
            "Epoch 28 - Training loss: 0.120\n",
            "On train set: Acc 0.955\n",
            "On test set: Acc 0.964\n",
            "Epoch 29 - Training loss: 0.119\n",
            "On train set: Acc 0.953\n",
            "On test set: Acc 0.788\n",
            "Epoch 30 - Training loss: 0.163\n",
            "On train set: Acc 0.923\n",
            "On test set: Acc 0.895\n",
            "Epoch 31 - Training loss: 0.123\n",
            "On train set: Acc 0.954\n",
            "On test set: Acc 0.891\n",
            "Epoch 32 - Training loss: 0.142\n",
            "On train set: Acc 0.950\n",
            "On test set: Acc 0.891\n",
            "Epoch 33 - Training loss: 0.110\n",
            "On train set: Acc 0.958\n",
            "On test set: Acc 0.882\n",
            "Epoch 34 - Training loss: 0.112\n",
            "On train set: Acc 0.952\n",
            "On test set: Acc 0.962\n",
            "Epoch 35 - Training loss: 0.125\n",
            "On train set: Acc 0.951\n",
            "On test set: Acc 0.919\n",
            "Epoch 36 - Training loss: 0.113\n",
            "On train set: Acc 0.961\n",
            "On test set: Acc 0.961\n",
            "Epoch 37 - Training loss: 0.095\n",
            "On train set: Acc 0.959\n",
            "On test set: Acc 0.951\n",
            "Epoch 38 - Training loss: 0.098\n",
            "On train set: Acc 0.969\n",
            "On test set: Acc 0.942\n",
            "Epoch 39 - Training loss: 0.091\n",
            "On train set: Acc 0.969\n",
            "On test set: Acc 0.971\n",
            "Epoch 40 - Training loss: 0.104\n",
            "On train set: Acc 0.957\n",
            "On test set: Acc 0.944\n",
            "Epoch 41 - Training loss: 0.098\n",
            "On train set: Acc 0.959\n",
            "On test set: Acc 0.945\n",
            "Epoch 42 - Training loss: 0.105\n",
            "On train set: Acc 0.968\n",
            "On test set: Acc 0.945\n",
            "Epoch 43 - Training loss: 0.109\n",
            "On train set: Acc 0.969\n",
            "On test set: Acc 0.955\n",
            "Epoch 44 - Training loss: 0.094\n",
            "On train set: Acc 0.961\n",
            "On test set: Acc 0.895\n",
            "Epoch 45 - Training loss: 0.098\n",
            "On train set: Acc 0.958\n",
            "On test set: Acc 0.909\n",
            "Epoch 46 - Training loss: 0.111\n",
            "On train set: Acc 0.966\n",
            "On test set: Acc 0.912\n",
            "Epoch 47 - Training loss: 0.074\n",
            "On train set: Acc 0.975\n",
            "On test set: Acc 0.968\n",
            "Epoch 48 - Training loss: 0.069\n",
            "On train set: Acc 0.977\n",
            "On test set: Acc 0.938\n",
            "Epoch 49 - Training loss: 0.063\n",
            "On train set: Acc 0.985\n",
            "On test set: Acc 0.957\n",
            "Epoch 50 - Training loss: 0.067\n",
            "On train set: Acc 0.978\n",
            "On test set: Acc 0.959\n",
            "Epoch 51 - Training loss: 0.096\n",
            "On train set: Acc 0.971\n",
            "On test set: Acc 0.924\n",
            "Epoch 52 - Training loss: 0.070\n",
            "On train set: Acc 0.977\n",
            "On test set: Acc 0.968\n",
            "Epoch 53 - Training loss: 0.076\n",
            "On train set: Acc 0.979\n",
            "On test set: Acc 0.957\n",
            "Epoch 54 - Training loss: 0.061\n",
            "On train set: Acc 0.980\n",
            "On test set: Acc 0.969\n",
            "Epoch 55 - Training loss: 0.061\n",
            "On train set: Acc 0.980\n",
            "On test set: Acc 0.976\n",
            "Epoch 56 - Training loss: 0.061\n",
            "On train set: Acc 0.983\n",
            "On test set: Acc 0.961\n",
            "Epoch 57 - Training loss: 0.059\n",
            "On train set: Acc 0.978\n",
            "On test set: Acc 0.947\n",
            "Epoch 58 - Training loss: 0.048\n",
            "On train set: Acc 0.980\n",
            "On test set: Acc 0.962\n",
            "Epoch 59 - Training loss: 0.051\n",
            "On train set: Acc 0.981\n",
            "On test set: Acc 0.975\n",
            "Epoch 60 - Training loss: 0.047\n",
            "On train set: Acc 0.987\n",
            "On test set: Acc 0.937\n",
            "Epoch 61 - Training loss: 0.059\n",
            "On train set: Acc 0.974\n",
            "On test set: Acc 0.933\n",
            "Epoch 62 - Training loss: 0.068\n",
            "On train set: Acc 0.971\n",
            "On test set: Acc 0.924\n",
            "Epoch 63 - Training loss: 0.088\n",
            "On train set: Acc 0.969\n",
            "On test set: Acc 0.938\n",
            "Epoch 64 - Training loss: 0.080\n",
            "On train set: Acc 0.971\n",
            "On test set: Acc 0.965\n",
            "Epoch 65 - Training loss: 0.048\n",
            "On train set: Acc 0.984\n",
            "On test set: Acc 0.912\n",
            "Epoch 66 - Training loss: 0.045\n",
            "On train set: Acc 0.988\n",
            "On test set: Acc 0.980\n",
            "Epoch 67 - Training loss: 0.055\n",
            "On train set: Acc 0.982\n",
            "On test set: Acc 0.966\n",
            "Epoch 68 - Training loss: 0.041\n",
            "On train set: Acc 0.985\n",
            "On test set: Acc 0.976\n",
            "Epoch 69 - Training loss: 0.036\n",
            "On train set: Acc 0.987\n",
            "On test set: Acc 0.921\n",
            "Epoch 70 - Training loss: 0.046\n",
            "On train set: Acc 0.985\n",
            "On test set: Acc 0.961\n",
            "Epoch 71 - Training loss: 0.042\n",
            "On train set: Acc 0.984\n",
            "On test set: Acc 0.969\n",
            "Epoch 72 - Training loss: 0.065\n",
            "On train set: Acc 0.983\n",
            "On test set: Acc 0.935\n",
            "Epoch 73 - Training loss: 0.088\n",
            "On train set: Acc 0.974\n",
            "On test set: Acc 0.968\n",
            "Epoch 74 - Training loss: 0.077\n",
            "On train set: Acc 0.977\n",
            "On test set: Acc 0.965\n",
            "Epoch 75 - Training loss: 0.063\n",
            "On train set: Acc 0.981\n",
            "On test set: Acc 0.849\n",
            "Epoch 76 - Training loss: 0.067\n",
            "On train set: Acc 0.982\n",
            "On test set: Acc 0.966\n",
            "Epoch 77 - Training loss: 0.055\n",
            "On train set: Acc 0.988\n",
            "On test set: Acc 0.966\n",
            "Epoch 78 - Training loss: 0.059\n",
            "On train set: Acc 0.979\n",
            "On test set: Acc 0.979\n",
            "Epoch 79 - Training loss: 0.049\n",
            "On train set: Acc 0.985\n",
            "On test set: Acc 0.982\n",
            "Epoch 80 - Training loss: 0.045\n",
            "On train set: Acc 0.989\n",
            "On test set: Acc 0.966\n",
            "Epoch 81 - Training loss: 0.043\n",
            "On train set: Acc 0.987\n",
            "On test set: Acc 0.955\n",
            "Epoch 82 - Training loss: 0.039\n",
            "On train set: Acc 0.988\n",
            "On test set: Acc 0.955\n",
            "Epoch 83 - Training loss: 0.038\n",
            "On train set: Acc 0.983\n",
            "On test set: Acc 0.975\n",
            "Epoch 84 - Training loss: 0.029\n",
            "On train set: Acc 0.990\n",
            "On test set: Acc 0.959\n",
            "Epoch 85 - Training loss: 0.034\n",
            "On train set: Acc 0.986\n",
            "On test set: Acc 0.958\n",
            "Epoch 86 - Training loss: 0.038\n",
            "On train set: Acc 0.985\n",
            "On test set: Acc 0.973\n",
            "Epoch 87 - Training loss: 0.028\n",
            "On train set: Acc 0.992\n",
            "On test set: Acc 0.978\n",
            "Epoch 88 - Training loss: 0.026\n",
            "On train set: Acc 0.993\n",
            "On test set: Acc 0.954\n",
            "Epoch 89 - Training loss: 0.041\n",
            "On train set: Acc 0.988\n",
            "On test set: Acc 0.968\n",
            "Epoch 90 - Training loss: 0.033\n",
            "On train set: Acc 0.989\n",
            "On test set: Acc 0.965\n",
            "Epoch 91 - Training loss: 0.034\n",
            "On train set: Acc 0.989\n",
            "On test set: Acc 0.954\n",
            "Epoch 92 - Training loss: 0.037\n",
            "On train set: Acc 0.991\n",
            "On test set: Acc 0.958\n",
            "Epoch 93 - Training loss: 0.032\n",
            "On train set: Acc 0.988\n",
            "On test set: Acc 0.964\n",
            "Epoch 94 - Training loss: 0.036\n",
            "On train set: Acc 0.990\n",
            "On test set: Acc 0.955\n",
            "Epoch 95 - Training loss: 0.035\n",
            "On train set: Acc 0.988\n",
            "On test set: Acc 0.919\n",
            "Epoch 96 - Training loss: 0.033\n",
            "On train set: Acc 0.989\n",
            "On test set: Acc 0.968\n",
            "Epoch 97 - Training loss: 0.029\n",
            "On train set: Acc 0.991\n",
            "On test set: Acc 0.975\n",
            "Epoch 98 - Training loss: 0.024\n",
            "On train set: Acc 0.994\n",
            "On test set: Acc 0.979\n",
            "Epoch 99 - Training loss: 0.019\n",
            "On train set: Acc 0.993\n",
            "On test set: Acc 0.951\n",
            "Epoch 100 - Training loss: 0.018\n",
            "On train set: Acc 0.993\n",
            "On test set: Acc 0.961\n",
            "Epoch 101 - Training loss: 0.024\n",
            "On train set: Acc 0.995\n",
            "On test set: Acc 0.972\n",
            "Epoch 102 - Training loss: 0.020\n",
            "On train set: Acc 0.996\n",
            "On test set: Acc 0.980\n",
            "Epoch 103 - Training loss: 0.021\n",
            "On train set: Acc 0.994\n",
            "On test set: Acc 0.965\n",
            "Epoch 104 - Training loss: 0.015\n",
            "On train set: Acc 0.994\n",
            "On test set: Acc 0.980\n",
            "Epoch 105 - Training loss: 0.029\n",
            "On train set: Acc 0.994\n",
            "On test set: Acc 0.980\n",
            "Epoch 106 - Training loss: 0.027\n",
            "On train set: Acc 0.992\n",
            "On test set: Acc 0.980\n",
            "Epoch 107 - Training loss: 0.022\n",
            "On train set: Acc 0.993\n",
            "On test set: Acc 0.980\n",
            "Epoch 108 - Training loss: 0.021\n",
            "On train set: Acc 0.994\n",
            "On test set: Acc 0.968\n",
            "Epoch 109 - Training loss: 0.016\n",
            "On train set: Acc 0.993\n",
            "On test set: Acc 0.952\n",
            "Epoch 110 - Training loss: 0.019\n",
            "On train set: Acc 0.995\n",
            "On test set: Acc 0.966\n",
            "Epoch 111 - Training loss: 0.018\n",
            "On train set: Acc 0.993\n",
            "On test set: Acc 0.961\n",
            "Epoch 112 - Training loss: 0.033\n",
            "On train set: Acc 0.993\n",
            "On test set: Acc 0.961\n",
            "Epoch 113 - Training loss: 0.029\n",
            "On train set: Acc 0.990\n",
            "On test set: Acc 0.959\n",
            "Epoch 114 - Training loss: 0.071\n",
            "On train set: Acc 0.983\n",
            "On test set: Acc 0.870\n",
            "Epoch 115 - Training loss: 0.115\n",
            "On train set: Acc 0.969\n",
            "On test set: Acc 0.947\n",
            "Epoch 116 - Training loss: 0.102\n",
            "On train set: Acc 0.960\n",
            "On test set: Acc 0.813\n",
            "Epoch 117 - Training loss: 0.089\n",
            "On train set: Acc 0.976\n",
            "On test set: Acc 0.846\n",
            "Epoch 118 - Training loss: 0.077\n",
            "On train set: Acc 0.972\n",
            "On test set: Acc 0.717\n",
            "Epoch 119 - Training loss: 0.076\n",
            "On train set: Acc 0.978\n",
            "On test set: Acc 0.697\n",
            "Epoch 120 - Training loss: 0.069\n",
            "On train set: Acc 0.980\n",
            "On test set: Acc 0.893\n",
            "Best Train Acc: 0.9957924263674615\n",
            "Best Test Acc: 0.9817671809256662\n"
          ]
        }
      ],
      "source": [
        "model = TFTwoDCNN(dropout=0.01)\n",
        "train(model, train_loader, test_loader, eval_loader, 120, 0.05)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "sitting_posture",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
