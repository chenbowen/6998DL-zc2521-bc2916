{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oglzm-Z3aif"
      },
      "source": [
        "### **Section 1: Data Prep**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJRtXZRRcJb2",
        "outputId": "00ebe493-bb37-4380-d1fe-358ef1df269c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting m2cgen\n",
            "  Downloading m2cgen-0.9.0-py3-none-any.whl (73 kB)\n",
            "Requirement already satisfied: numpy in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from m2cgen) (1.19.5)\n",
            "Installing collected packages: m2cgen\n",
            "Successfully installed m2cgen-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install m2cgen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6AAmwSO6wpR"
      },
      "source": [
        "#### **1.1 Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzMdhEEt0dRR",
        "outputId": "247aaddd-d89c-4d0c-d0c1-7f72a36830d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Collecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-win_amd64.whl (6.8 MB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from scikit-learn==0.24.2) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from scikit-learn==0.24.2) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from scikit-learn==0.24.2) (1.6.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from scikit-learn==0.24.2) (2.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "Successfully installed scikit-learn-1.0\n"
          ]
        }
      ],
      "source": [
        "# Install the Updated Version of SKLearn if Nedded\n",
        "!pip install scikit-learn==0.24.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "g-fRgFfnS1NR",
        "outputId": "c7f9ceea-5d09-4dd1-dd7f-1447a20a40c5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use(\"ggplot\")\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from PIL import Image as im\n",
        "import sys\n",
        "sys.setrecursionlimit(2147483647)\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import pandas as pd\n",
        "style.use(\"ggplot\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load Training Data and Evaluation Data.\n",
        "def data_prep_multiclass(path_prefix):\n",
        "    results = []\n",
        "    for name in [\"eval\", \"train\"]:\n",
        "        csv = pd.read_csv(f\"{path_prefix}{name}.csv\")\n",
        "        csv = csv.iloc[:, 0:258]\n",
        "        csv.dropna(inplace=True)\n",
        "        csv.drop_duplicates(inplace=True)\n",
        "        csv.reset_index(drop=True,inplace=True)\n",
        "        results.append(csv)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "o2LQ6xa5lyHl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3565,)\n",
            "(3565, 1, 16, 16)\n",
            "(35166,)\n",
            "(35166, 1, 16, 16)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0., 1., 2., 3., 4., 5., 6.])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_csv, eval_csv = data_prep_multiclass(\"data/1126\")\n",
        "assert len(train_csv.label.unique()) == len(eval_csv.label.unique())\n",
        "num_class = len(train_csv.label.unique())\n",
        "\n",
        "class PandasDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.transform = transform\n",
        "        self.dataframe = dataframe\n",
        "        self.labels = self.dataframe.label.to_numpy().astype(float)\n",
        "        self.data = self.dataframe.iloc[:, 1:257].to_numpy()\n",
        "        self.data = (self.data - self.data.mean(1, keepdims=True)) / (self.data.std(1, keepdims=True) + 1e-8)\n",
        "        # self.data = (self.data - self.data.mean(0, keepdims=True)) / (self.data.std(0, keepdims=True) + 1e-8)\n",
        "        self.data = self.data.reshape(-1, 1, 16, 16).astype(float)\n",
        "        print(self.labels.shape)\n",
        "        print(self.data.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.data[index, ...]\n",
        "        if self.transform:\n",
        "            img = self.transform(Image.fromarray(img[0, ...]))\n",
        "        target = float(self.labels[index])\n",
        "        return img, target\n",
        "\n",
        "batch_size_train = 1280\n",
        "batch_size_test = 1280\n",
        "total_size  = len(train_csv)\n",
        "train_size = int(total_size * 0.8)\n",
        "test_size = total_size - train_size\n",
        "# train_dataset, test_dataset = random_split(PandasDataset(train_csv, transform=transforms),\n",
        "train_dataset, test_dataset = random_split(PandasDataset(train_csv, transform=None),\n",
        "                                           [train_size, test_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=True)\n",
        "eval_loader = torch.utils.data.DataLoader(PandasDataset(eval_csv,transform=None), \n",
        "                                          batch_size=len(eval_csv), shuffle=False)\n",
        "train_csv.label.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "0YiajcKjkYpN",
        "outputId": "9fe2e1a9-4ffa-4915-fe94-692aa0b26c93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1280, 1, 16, 16])\n",
            "torch.Size([1280])\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAELCAYAAABj+Hm+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAc0klEQVR4nO3df0zV9R7H8dcBpIOagBcNTAdEXkVQIK2Jk9Sb61rT7h/t0rrTC8Xyrl2b2l1emTrMLGfp1FVsLZum3hXV2lrmrwlu6WZpgZqYJSSNJhLNO7jTrhjnc/9wnuuB7zkc8JzDB3w+Njb4nO+Pz/fw5rzOF958vy5jjBEAABaK6usJAADgDyEFALAWIQUAsBYhBQCwFiEFALAWIQUAsNZtEVIul0u7du3q62lIkhoaGuRyuXTkyJG+ngqCQO2gN6ib0AkqpC5duqTS0lJNmDBBgwcPVmJionJzc7VixQo1NjaGe45hNXPmTLlcroAfDQ0Nvdr27NmzVVxcHNL53mz//v3Kz89XUlKS3G63MjIytHLlSrW3t3e77quvvqrU1FS53W7l5eXpwIEDYZkjtdPQq22Hu3ZudubMGQ0ZMkQxMTFBLR+J2qFuGnq17XDXzfbt2x3ne/DgwW7X7W3ddFuVjY2Nmj59umJiYrR69Wrl5OTI7Xarvr5en3zyiTZs2KAtW7Y4rtve3q7Y2NigJtJXPv74Y58X9dGjR2vjxo164oknvGMjRozwfm7TMQ0bNkyLFy9Wdna27rzzTtXU1GjhwoW6fPmyNm3a5He9zZs3q6ysTG+99Zbuv/9+bdu2TfPmzdPx48c1adKkkM2P2rG3dm64cuWKCgsL9Yc//EF79+7tdvlI1A51Y3fdREdH66effvIZGz58eMB1bqluTDfmzp1rkpOTTWtrq+PjHo/H+/mMGTPM008/bVauXGmSk5NNUlKSMcaYo0ePmoKCAuN2u01CQoJ58sknTXNzs3e9srIyk5GR4bPdw4cPG0nm/Pnzxhhjtm3bZqKjo82RI0dMXl6eiYuLM1OmTDFfffWVz3pVVVVm4sSJ5o477jATJ040VVVVRpLZuXNnd4dqjDEmOjrabNu2rdtjSk1NNS+99JLPuiUlJWbGjBnGGGOKioqMJJ+PQ4cOmfPnzxtJpqKiwsydO9fExcWZ9PR0s2PHjqDm150lS5aY3Nxcv497PB4zatQoU1pa6jM+ZcoUU1RUFJI53EDt2F87xcXF5m9/+5v3OQokUrVD3dhbN8HUSWe3WjcBf9136dIl7dmzR88995yGDRvmuIzL5fL5+oMPPlBLS4sqKytVVVWlixcv6uGHH9bo0aN17Ngxffrppzp9+rQef/zxwOnpwOPxqLS0VFu2bFF1dbUSExNVWFio3377TZJ04cIFzZ07V5MnT1Z1dbU2btyoxYsX93g/nXU+pmBs2bJFBQUFKiwsVFNTk5qamjRt2jTv48uXL9eCBQt06tQpFRYW6qmnntK5c+e8jxcXFystLa1H8zx79qz27t2rWbNm+V2moaFBFy5c0Jw5c3zG58yZE9LfWVM7zscUjEjVzo4dO3T8+PGAZ903i0TtUDfOxxSMSNVNR0eH7rnnHqWkpGjmzJnavXt3wOVvtW4C/rqvrq5OHo9HmZmZPuPTpk3TqVOnJEmpqamqra31PpaSkqLy8nJFRV3Pv1WrVmnYsGHavn2795R1586dys3N1eeff64HH3yw20neYIzR5s2bdd9990mS1qxZo/z8fNXX12vcuHEqLy9XUlKS3n77bcXExGjChAl65ZVXNG/evKD34aTzMQUjPj5esbGxiouLU3JycpfHFy1apMLCQknS2rVr9cYbb6iqqkpjx4717jMjIyOofY0ePVotLS1qb2/XwoUL9dprr/ldtqmpSZK6zCk5Odn7WChQO87HFIxI1M63336rf/zjHzp06JDi4uKCmlckaoe6cT6mYESibsaNG6d3331XkyZN0q+//qqKigrNmzdPW7duVUlJieM6t1o3AZ8B4+fasxUVFTpx4oT37x83mzx5ss8TW1tbq6lTp/r8TjUnJ0fx8fE+hRYMl8ulnJwc79d33323JKm5uVnS9T8AP/DAAz5/AJ4+fXqP9uGk8zGFQm5urvfzmJgY3XXXXd7jkKR169apsrIyqG0dPnxY1dXV2rlzp3bv3q01a9b0ak6d36HeCmrnOhtr5+rVq/rzn/+stWvXKjs7OyRzClXtUDfX2Vg3kpSfn6+//vWvys3NVX5+vjZv3qwFCxZo/fr1vZpTMHUT8FkYO3asoqKidObMGZ/xMWPG6N5773X8Y9mQIUOCnsiN8aioqC7Fee3ata6TjYpSdHR0l/U9Ho+k6wXeeV+h+OFxOqZg5+xP5z+Eulwu73H0VHp6urKysjR//nxt2LBBa9eu7fKDfENKSook6eLFiz7jzc3Nju++eovauc7G2mlqalJtba3+/ve/KyYmRjExMSopKVFHR4diYmL0yiuvOK4Xidqhbq6zsW78mTZtWsBuxFutm4AhNXz4cD3yyCN6/fXX1draGsR0u8rKytLRo0d9ullOnjyp1tZWZWVlSZJGjhypn3/+WR0dHd5lqqure7WvL7/80mc74frfgJEjR+rChQs+YzU1NT5fx8bG+swlEjwejzwej9/iTUtL06hRo7R//36f8X379oXkHeAN1I5/fV07d999t7755hudOHHC+7FmzRpFR0frxIkTeuaZZxzXi0TtUDf+9XXd+FNTU6MxY8b4ffxW66bb88ny8nINGjRIeXl52rFjh06dOqUffvhBe/fu1e7du33eZThZtGiR2traVFxcrNOnT+vIkSNasGCBpk+froKCAknSrFmzdOXKFa1atUr19fX68MMP9eabb3Y7+c6effZZtbS0aOHChfr2229VWVmpFStW9Hg7wZg9e7YqKip04MABfffdd1q6dKl+/PFHn2XS09P19ddfq76+Xr/88kuP3vWUlpbqoYceCrjMxo0b9dlnn+ncuXOqq6vT+++/r2XLlumxxx5TQkKCJOnYsWMaP368jh07Jun6u6cXXnhBmzZt0q5du3T27FktX75cJ0+e1NKlS3v2JHSD2nHW17UzaNAgZWdn+3zc+DVWdna2t/25r2qHunHW13UjSatXr9aePXtUV1en2tpavfjii9q6dauef/557zIhr5tgWghbWlrMsmXLzPjx443b7TZut9tkZmaaJUuWeNs1jbneOllSUtJl/ZvbQePj47u0gxpjzDvvvGPS09ON2+02c+bMMe+9955jO+jNGhsbvW2WNxw8eNBkZ2eb2NhYk5WVZSorK2+5HdTpmNra2sz8+fNNQkKCGTFihCkrK/NpBzXGmPr6elNQUGCGDBnSpR308OHDPtvLyMgwZWVl3q+LiopMampqwLmuW7fOZGZmmsGDB5uhQ4earKws8/LLL5vLly97lzl06FCX58gYY9avX2/GjBljYmNjTU5Ojtm3b1+3z01vUDt21k5nTs9RX9YOdWNn3SxdutSkpaUZt9ttEhMTTX5+vvnoo498lgl13biM4c68AAA73RbX7gMA9E+EFADAWoQUAMBahBQAwFqEFADAWoQUAMBawd3lDD78XfaEbv7+i+8p0HP+/rE6lFe94EwKAGAtQgoAYC1CCgBgLf4mBQC34MZdeju7+R5TA1UobvXRHc6kAADWIqQAANYipAAA1iKkAADWIqQAANYipAAA1hr4PZIAEEa3Q6u5P5E4ds6kAADWIqQAANYipAAA1iKkAADWIqQAANYipAAA1iKkAADWIqQAANYipAAA1iKkAADWIqQAANYKy4WXjDHh2Kz1bDxul8vV11MImo3Pn41zihRqB92JjY0N+z44kwIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFjr9r3vMQDglkRFhf88hzMpAIC1CCkAgLUIKQCAtQgpAIC1CCkAgLWs7u7zd4FLLiYJ4Gb96WK4A8lvv/0W9n1wJgUAsBYhBQCwFiEFALAWIQUAsBYhBQCwltXdfXTxAQiG02tFoNeP/tYN6O9Y+vo4oqOjw74PzqQAANYipAAA1iKkAADWIqQAANYipAAA1rK6uw+3l77uVHLS0zn1piO1N8dta7eXTQbSc2HrsXR0dIR9H5xJAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArGVFC3ok2nz7UqDj62/HgsAi1Spsa0syEGqcSQEArEVIAQCsRUgBAKxFSAEArEVIAQCsZUV330DvcBvoxxcqNj5PNs4pUuggRHeiosJ/nsOZFADAWoQUAMBahBQAwFqEFADAWoQUAMBaVnT3AQD6n0h0v3ImBQCwFiEFALAWIQUAsBYhBQCwFiEFALAWIQUAsFZYWtA9Hk+XsVBerLI3bY+hvEW9v231Zl6hbOF0mld/ukhouNtZI/H9iURt9mY/gfbhb1vR0dE92kdfcnrNQfgNHjw47PvgTAoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgrYhdYLavb8Mdif3fDscYTjbOf6DUjY3PLfq/SHQPcyYFALAWIQUAsBYhBQCwFiEFALAWIQUAsFZYuvv60/XiesNfp9RAP+5wu/POO7uMxcbG9ng7//3vfx3Hc3JyerytRx991HF80KBBjuOVlZV+t/XAAw84jj/11FOO47t27fK7rS+++MJxPD4+3nE80DXW/v3vfzuO//LLL37XsY3Tz15vrr8ZCZHotOzNtRp7o729PWTb8oczKQCAtQgpAIC1CCkAgLUIKQCAtQgpAIC1CCkAgLVcJgz9kAP9Vs7+2o+vXbsW4Zl0Lyqq/7wP6cu68Xer9I6ODsdxfy2+/fFCrv6OpT/9S4W/7xPCKzk52XG8paUlZPvoP69gAIDbDiEFALAWIQUAsBYhBQCwFiEFALBWWLr7AAAIBc6kAADWIqQAANYipAAA1iKkAADWIqQAANYipAAA1iKkAADWIqQAANYipAAA1iKkAADWIqQAANYipAAA1iKkAADWIqQAANa6LULK5XJp165dfT0NSVJDQ4NcLpeOHDnS11NBEKgd9AZ1EzpBhdSlS5dUWlqqCRMmaPDgwUpMTFRubq5WrFihxsbGcM8xrGbOnCmXyxXwo6GhoVfbnj17toqLi0M635vt379f+fn5SkpKktvtVkZGhlauXKn29vZu13311VeVmpoqt9utvLw8HThwICxzpHYaerXtcNfO9u3bHed78ODBbteNRO1QNw292vZArJuY7hZobGzU9OnTFRMTo9WrVysnJ0dut1v19fX65JNPtGHDBm3ZssVx3fb2dsXGxgY1kb7y8ccf+7yojx49Whs3btQTTzzhHRsxYoT3c5uOadiwYVq8eLGys7N15513qqamRgsXLtTly5e1adMmv+tt3rxZZWVleuutt3T//fdr27Ztmjdvno4fP65JkyaFbH7Ujr21I0nR0dH66aeffMaGDx8ecJ1I1A51Q934MN2YO3euSU5ONq2trY6Pezwe7+czZswwTz/9tFm5cqVJTk42SUlJxhhjjh49agoKCozb7TYJCQnmySefNM3Nzd71ysrKTEZGhs92Dx8+bCSZ8+fPG2OM2bZtm4mOjjZHjhwxeXl5Ji4uzkyZMsV89dVXPutVVVWZiRMnmjvuuMNMnDjRVFVVGUlm586d3R2qMcaY6Ohos23btm6PKTU11bz00ks+65aUlJgZM2YYY4wpKioyknw+Dh06ZM6fP28kmYqKCjN37lwTFxdn0tPTzY4dO4KaX3eWLFlicnNz/T7u8XjMqFGjTGlpqc/4lClTTFFRUUjmcAO1Y2/t3HhOeiJStUPdUDc3C/jrvkuXLmnPnj167rnnNGzYMMdlXC6Xz9cffPCBWlpaVFlZqaqqKl28eFEPP/ywRo8erWPHjunTTz/V6dOn9fjjjwdOTwcej0elpaXasmWLqqurlZiYqMLCQv3222+SpAsXLmju3LmaPHmyqqurtXHjRi1evLjH++ms8zEFY8uWLSooKFBhYaGamprU1NSkadOmeR9fvny5FixYoFOnTqmwsFBPPfWUzp075328uLhYaWlpPZrn2bNntXfvXs2aNcvvMg0NDbpw4YLmzJnjMz5nzpyQ/s6a2nE+pmBEqnY6Ojp0zz33KCUlRTNnztTu3bsDLh+J2qFunI8pGAO1bgL+uq+urk4ej0eZmZk+49OmTdOpU6ckSampqaqtrfU+lpKSovLyckVFXc+/VatWadiwYdq+fbv3lHXnzp3Kzc3V559/rgcffLDbSd5gjNHmzZt13333SZLWrFmj/Px81dfXa9y4cSovL1dSUpLefvttxcTEaMKECXrllVc0b968oPfhpPMxBSM+Pl6xsbGKi4tTcnJyl8cXLVqkwsJCSdLatWv1xhtvqKqqSmPHjvXuMyMjI6h9jR49Wi0tLWpvb9fChQv12muv+V22qalJkrrMKTk52ftYKFA7zscUjEjUzrhx4/Tuu+9q0qRJ+vXXX1VRUaF58+Zp69atKikpcVwnErVD3TgfUzAGat0EDCljjON4RUWFrl69qvLycn388cc+j02ePNnnia2trdXUqVN9fqeak5Oj+Ph41dbW9qhgXC6XcnJyvF/ffffdkqTm5maNGzdOZ86c0QMPPKCYmP8f1vTp04Pevj+djykUcnNzvZ/HxMTorrvuUnNzs3ds3bp1QW/r8OHDunLlimpqavTPf/5TycnJevHFF3s8p87vUG8FtXOdrbWTn5+v/Px8n68vXbqk9evX+32xCSRUtUPdXEfd/F/AZ2Hs2LGKiorSmTNnfMbHjBmje++91/GPZUOGDAl6IjfGo6KiuhTntWvXuk42KkrR0dFd1vd4PJKuF3jnfYXih8fpmIKdsz+d/xDqcrm8x9FT6enpysrK0vz587VhwwatXbtWly9fdlw2JSVFknTx4kWf8ebmZsd3X71F7Vxne+3cbNq0aQG7yiJRO9TNddTN/wUMqeHDh+uRRx7R66+/rtbW1iCm21VWVpaOHj3q081y8uRJtba2KisrS5I0cuRI/fzzz+ro6PAuU11d3at9ffnllz7bCdf/BowcOVIXLlzwGaupqfH5OjY21mcukeDxeOTxePwWb1pamkaNGqX9+/f7jO/bty8k7wBvoHb8s7V2ampqNGbMGL+PR6J2qBv/bte66fZ8sry8XIMGDVJeXp527NihU6dO6YcfftDevXu1e/dun3cZThYtWqS2tjYVFxfr9OnTOnLkiBYsWKDp06eroKBAkjRr1ixduXJFq1atUn19vT788EO9+eab3U6+s2effVYtLS1auHChvv32W1VWVmrFihU93k4wZs+erYqKCh04cEDfffedli5dqh9//NFnmfT0dH399deqr6/XL7/80qN3PaWlpXrooYcCLrNx40Z99tlnOnfunOrq6vT+++9r2bJleuyxx5SQkCBJOnbsmMaPH69jx45Juv7u6YUXXtCmTZu0a9cunT17VsuXL9fJkye1dOnSnj0J3aB2nNlQO6tXr9aePXtUV1en2tpavfjii9q6dauef/557zJ9VTvUjbPbtm6CaSFsaWkxy5YtM+PHjzdut9u43W6TmZlplixZ4m3XNOZ662RJSUmX9W9uB42Pj+/SDmqMMe+8845JT083brfbzJkzx7z33nuO7aA3a2xs9LZZ3nDw4EGTnZ1tYmNjTVZWlqmsrLzldlCnY2prazPz5883CQkJZsSIEaasrMynHdQYY+rr601BQYEZMmRIl3bQw4cP+2wvIyPDlJWVeb8uKioyqampAee6bt06k5mZaQYPHmyGDh1qsrKyzMsvv2wuX77sXebQoUNdniNjjFm/fr0ZM2aMiY2NNTk5OWbfvn3dPje9Qe3YWTtLly41aWlpxu12m8TERJOfn28++ugjn2X6snaoG+rmBpcxfv5SCQBAH7strt0HAOifCCkAgLUIKQCAtQgpAIC1CCkAgLUIKQCAtbq9n1RvvPPOO13GioqKwrGrsAnUmR/Ka9yF283XFAOcTJw40XH8m2++ifBM+id/rwe3w3/3DBo0yHG8J/9E3B3OpAAA1iKkAADWIqQAANYKyx8sTpw40WWsv/1Nqj/93Qm4FTfuMtuf/eUvf+ky1tbW5nf5m6+QfrPExETH8QkTJvR4TqtXr3Ycv3LliuP46dOn/W7rxk0XO/v+++8dx5OSkvxuq7Gx0XHc39XTO9/i42ahuNVHdziTAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWCssd+YtLy/vMvbMM8/4n0QfXlZkIF3SxOlYuCxScL744gvH8alTp0Z4JpGXl5fnOF5TUxPhmfRPA+k1pKe4LBIA4LZGSAEArEVIAQCsRUgBAKxFSAEArBWW1q+hQ4f2aPm+7IIZSB04A+lYIi0/P99x/HZ4TidPntzXUwD84kwKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYK2wXLuvqampy9jvfvc7/5MI4a3a/R1OT/fRm6elN8fR0/0E2ofTtgLd+tk2fdlJFxXl/H4tErfH7mvnzp1zHP/9738f4Zn0Xih/jnq6/Z7WzkC61p/b7XYcv3r1asj2wZkUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWmG5wOy//vWvLmOLFy8Ox666CFU7eyjb4iO1n0jNGQNLf2o19yfctd+b7fflz+NAei3gTAoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgrbB096Wnp4djsxjgbOxIsnFOGJj6Y635u7huSPcR9j0AANBLhBQAwFqEFADAWoQUAMBahBQAwFph6e5LTEzsMhbo1siRuJ3yQLpl80Bl4/fCxjlFSn/qNrPx+2TjnELN4/GEfR+cSQEArEVIAQCsRUgBAKxFSAEArEVIAQCsFZbuvuPHj3cZKygo6PF2ItFd1J86mAY6G78Xfdl5Goi//dv4HEaCjcdt45xCLRIdjJxJAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArBWWFvRIXHQQ6Et93V7c1/sHIoUzKQCAtQgpAIC1CCkAgLUIKQCAtQgpAIC1wtLdl56eHo7NRlRvbnffm231dB+hnBcA2I4zKQCAtQgpAIC1CCkAgLUIKQCAtQgpAIC1wtLd19bWFo7NhkVvOu8icevu3uwjErdyDien+ffmeEN5y/f+/pzeCrpFI6uvO3cj8brWG5xJAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArBWWFvSZM2d2GbsdbikfiXblgdwS7XRsgeqmp63mkWhBD+X3J5T/btCbNuK+bj3uCRtfX0I5p778uQ+076io8J/ncCYFALAWIQUAsBYhBQCwFiEFALAWIQUAsFZYuvtGjhwZjs1GVChv+Y7g9GUnnY1CeXyBtvXHP/7RcfzgwYMh2z/QW5xJAQCsRUgBAKxFSAEArEVIAQCsRUgBAKwVlu6+H374octYVlZWOHYVNnTqRd7UqVO7jJ0/f97v8oMHD3Ycj46OdhzPzc3t8ZyKioocx+vq6hzH6+vr/W4rIyPDcfytt95yHH/00Uf9bqu1tdVxPDY21nF8+PDhfrcVaM79xZ/+9KcuY19//bXf5UeMGOE43t7e7jg+efLkHs/piSeecBxva2tzHA8033vvvddx/L777nMc/+STT/xu69q1a47jHR0djuPDhg3zu62rV6/6fSxUOJMCAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYy2XCcJVOp1sKx8XF+V3eXyvxf/7zH8fxQG2PaWlpjuNDhw51HE9ISHAc//777/3uIyUlxXH88uXLjuP+jkOSLl686Dju7znJzMz0uy0ngdpabdOXF4z1dxtsf7cA7+mt6202fvx4x/HvvvsuwjPpvf5UO/709UWt/e0/0D7cbrfjeChb0zmTAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFgrLN19AACEAmdSAABrEVIAAGsRUgAAaxFSAABrEVIAAGsRUgAAaxFSAABrEVIAAGsRUgAAaxFSAABr/Q+qteFsZD51EAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "examples = enumerate(train_loader)\n",
        "idx, example_data = next(examples)\n",
        "imgs, labels = example_data\n",
        "print(imgs.shape)\n",
        "print(labels.shape)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(imgs[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(labels[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPsy2lAonH4_"
      },
      "source": [
        "#### **2.2 NN Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwoDCNN(nn.Module):\n",
        "  def __init__(self, config=[1, 4, 8, 16, 32], num_class=7, dropout=0.5):\n",
        "    super(TwoDCNN, self).__init__()\n",
        "    \n",
        "    modules = []\n",
        "    for i, dim in enumerate(config[1:-1]):\n",
        "      modules += [\n",
        "        nn.Conv2d(config[i], dim, 3, padding=1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.BatchNorm2d(dim),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.MaxPool2d(2)\n",
        "      ]\n",
        "    modules += [\n",
        "      nn.Conv2d(config[-2], config[-1], 3, padding=1),\n",
        "      nn.BatchNorm2d(config[-1]),\n",
        "      nn.AdaptiveAvgPool2d((1, 1)),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(config[-1], num_class),\n",
        "    ]\n",
        "    self.net = nn.Sequential(*modules)\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.net:\n",
        "        x = layer(x)\n",
        "    return x\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    \n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h = self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b p h n d -> b p n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads, dim_head, dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
        "            ]))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileViTBlock(nn.Module):\n",
        "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.ph, self.pw = patch_size\n",
        "\n",
        "        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
        "        self.conv2 = conv_1x1_bn(channel, dim)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, 1, 32, mlp_dim, dropout)\n",
        "\n",
        "        self.conv3 = conv_1x1_bn(dim, channel)\n",
        "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        y = x.clone()\n",
        "\n",
        "        # Local representations\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        # Global representations\n",
        "        _, _, h, w = x.shape\n",
        "        x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)\n",
        "        x = self.transformer(x)\n",
        "        x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h//self.ph, w=w//self.pw, ph=self.ph, pw=self.pw)\n",
        "\n",
        "        # Fusion\n",
        "        x = self.conv3(x)\n",
        "        x = torch.cat((x, y), 1)\n",
        "        x = self.conv4(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# our mobilevit\n",
        "class TFTwoDCNN(nn.Module):\n",
        "  def __init__(self, config=[1, 4, 8, 16, 16], maxpool=[True, True, True], \n",
        "               num_class=7, dropout=0.5):\n",
        "    super(TFTwoDCNN, self).__init__()\n",
        "    \n",
        "    modules = []\n",
        "    for i, dim in enumerate(config[1:-1]):\n",
        "      modules += [\n",
        "        nn.Conv2d(config[i], dim, 3, padding=1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.BatchNorm2d(dim),\n",
        "        nn.Dropout(dropout)\n",
        "      ]\n",
        "      if maxpool[i]:\n",
        "        modules += [nn.MaxPool2d(2)]\n",
        "      modules += [MobileViTBlock(dim, 2, dim, 3, (2, 2), dim*3)]\n",
        "    modules += [\n",
        "      nn.Conv2d(config[-2], config[-1], 3, padding=1),\n",
        "      nn.BatchNorm2d(config[-1]),\n",
        "      nn.AdaptiveAvgPool2d((1, 1)),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(config[-1], num_class),\n",
        "    ]\n",
        "    self.net = nn.Sequential(*modules)\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.net:\n",
        "        x = layer(x)\n",
        "    return x\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7E1xhwyCK1ia",
        "outputId": "4bdbad55-c6e3-4230-8e58-b908de73d77c"
      },
      "outputs": [],
      "source": [
        "def test_eval(model, test_loader, eval_loader):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model.eval()\n",
        "  acc = {}\n",
        "  with torch.no_grad():\n",
        "    # for name, loader in [(\"test\", test_loader), (\"eval\", eval_loader)]:\n",
        "    for name, loader in [(\"test\", test_loader)]:\n",
        "      correct_count, all_count = 0, 0\n",
        "      for images, labels in loader:\n",
        "        images = images.type(torch.float).to(device)\n",
        "        labels = labels.type(torch.long).to(device)\n",
        "        out = model(images)\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "        correct_count += (labels == pred).sum().item()\n",
        "        all_count += images.shape[0]\n",
        "      acc[name] = correct_count / all_count\n",
        "      print(f\"On %s set: Acc %.3f\" % (\n",
        "        name, acc[name]))\n",
        "  # return acc[\"test\"], acc[\"eval\"]\n",
        "  return acc[\"test\"], 0\n",
        "\n",
        "def train(model, train_loader, test_loader, eval_loader, epochs, lr):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = model.to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "  eval_acc_list = []\n",
        "  train_acc_list = []\n",
        "  test_acc_list = []\n",
        "  for e in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    ct, all_count = 0, 0\n",
        "    for images, labels in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      images = images.type(torch.float).to(device)\n",
        "      labels = labels.type(torch.long).to(device)\n",
        "      output = model(images)\n",
        "      loss = criterion(output, labels)\n",
        "      pred = torch.argmax(output, dim=1)\n",
        "      ct += (labels == pred).sum().item()\n",
        "      all_count += images.shape[0]\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      ct += 1\n",
        "      running_loss += loss.item()\n",
        "    train_acc_list.append(ct / all_count)\n",
        "    print(\"Epoch {} - Training loss: {:.3f}\".format(e+1, running_loss/len(train_loader)))\n",
        "    print(f\"On train set: Acc %.3f\" % (train_acc_list[-1]))\n",
        "    test_acc, eval_acc = test_eval(model, test_loader, eval_loader)\n",
        "    eval_acc_list.append(test_acc)\n",
        "    # test_acc_list.append(eval_acc)\n",
        "  print(f\"Best Train Acc: {max(train_acc_list)}\")\n",
        "  print(f\"Best Test Acc: {max(eval_acc_list)}\")\n",
        "  # print(f\"Best Eval Acc: {max(test_acc_list)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Training loss: 1.674\n",
            "On train set: Acc 0.291\n",
            "On test set: Acc 0.217\n",
            "Epoch 2 - Training loss: 1.054\n",
            "On train set: Acc 0.592\n",
            "On test set: Acc 0.474\n",
            "Epoch 3 - Training loss: 0.805\n",
            "On train set: Acc 0.706\n",
            "On test set: Acc 0.546\n",
            "Epoch 4 - Training loss: 0.676\n",
            "On train set: Acc 0.750\n",
            "On test set: Acc 0.640\n",
            "Epoch 5 - Training loss: 0.596\n",
            "On train set: Acc 0.781\n",
            "On test set: Acc 0.683\n",
            "Epoch 6 - Training loss: 0.557\n",
            "On train set: Acc 0.793\n",
            "On test set: Acc 0.746\n",
            "Epoch 7 - Training loss: 0.512\n",
            "On train set: Acc 0.797\n",
            "On test set: Acc 0.753\n",
            "Epoch 8 - Training loss: 0.461\n",
            "On train set: Acc 0.816\n",
            "On test set: Acc 0.743\n",
            "Epoch 9 - Training loss: 0.436\n",
            "On train set: Acc 0.835\n",
            "On test set: Acc 0.801\n",
            "Epoch 10 - Training loss: 0.377\n",
            "On train set: Acc 0.864\n",
            "On test set: Acc 0.783\n",
            "Epoch 11 - Training loss: 0.382\n",
            "On train set: Acc 0.854\n",
            "On test set: Acc 0.801\n",
            "Epoch 12 - Training loss: 0.367\n",
            "On train set: Acc 0.854\n",
            "On test set: Acc 0.808\n",
            "Epoch 13 - Training loss: 0.345\n",
            "On train set: Acc 0.874\n",
            "On test set: Acc 0.762\n",
            "Epoch 14 - Training loss: 0.352\n",
            "On train set: Acc 0.881\n",
            "On test set: Acc 0.878\n",
            "Epoch 15 - Training loss: 0.319\n",
            "On train set: Acc 0.889\n",
            "On test set: Acc 0.826\n",
            "Epoch 16 - Training loss: 0.341\n",
            "On train set: Acc 0.883\n",
            "On test set: Acc 0.812\n",
            "Epoch 17 - Training loss: 0.310\n",
            "On train set: Acc 0.906\n",
            "On test set: Acc 0.826\n",
            "Epoch 18 - Training loss: 0.297\n",
            "On train set: Acc 0.906\n",
            "On test set: Acc 0.874\n",
            "Epoch 19 - Training loss: 0.289\n",
            "On train set: Acc 0.918\n",
            "On test set: Acc 0.858\n",
            "Epoch 20 - Training loss: 0.242\n",
            "On train set: Acc 0.918\n",
            "On test set: Acc 0.826\n",
            "Epoch 21 - Training loss: 0.287\n",
            "On train set: Acc 0.915\n",
            "On test set: Acc 0.837\n",
            "Epoch 22 - Training loss: 0.251\n",
            "On train set: Acc 0.920\n",
            "On test set: Acc 0.863\n",
            "Epoch 23 - Training loss: 0.244\n",
            "On train set: Acc 0.919\n",
            "On test set: Acc 0.893\n",
            "Epoch 24 - Training loss: 0.263\n",
            "On train set: Acc 0.913\n",
            "On test set: Acc 0.865\n",
            "Epoch 25 - Training loss: 0.250\n",
            "On train set: Acc 0.908\n",
            "On test set: Acc 0.840\n",
            "Epoch 26 - Training loss: 0.235\n",
            "On train set: Acc 0.926\n",
            "On test set: Acc 0.898\n",
            "Epoch 27 - Training loss: 0.209\n",
            "On train set: Acc 0.934\n",
            "On test set: Acc 0.884\n",
            "Epoch 28 - Training loss: 0.199\n",
            "On train set: Acc 0.935\n",
            "On test set: Acc 0.899\n",
            "Epoch 29 - Training loss: 0.198\n",
            "On train set: Acc 0.932\n",
            "On test set: Acc 0.927\n",
            "Epoch 30 - Training loss: 0.166\n",
            "On train set: Acc 0.939\n",
            "On test set: Acc 0.906\n",
            "Epoch 31 - Training loss: 0.203\n",
            "On train set: Acc 0.938\n",
            "On test set: Acc 0.909\n",
            "Epoch 32 - Training loss: 0.190\n",
            "On train set: Acc 0.933\n",
            "On test set: Acc 0.903\n",
            "Epoch 33 - Training loss: 0.198\n",
            "On train set: Acc 0.932\n",
            "On test set: Acc 0.903\n",
            "Epoch 34 - Training loss: 0.191\n",
            "On train set: Acc 0.939\n",
            "On test set: Acc 0.842\n",
            "Epoch 35 - Training loss: 0.196\n",
            "On train set: Acc 0.931\n",
            "On test set: Acc 0.923\n",
            "Epoch 36 - Training loss: 0.170\n",
            "On train set: Acc 0.938\n",
            "On test set: Acc 0.909\n",
            "Epoch 37 - Training loss: 0.195\n",
            "On train set: Acc 0.944\n",
            "On test set: Acc 0.851\n",
            "Epoch 38 - Training loss: 0.224\n",
            "On train set: Acc 0.940\n",
            "On test set: Acc 0.863\n",
            "Epoch 39 - Training loss: 0.195\n",
            "On train set: Acc 0.940\n",
            "On test set: Acc 0.882\n",
            "Epoch 40 - Training loss: 0.202\n",
            "On train set: Acc 0.930\n",
            "On test set: Acc 0.885\n",
            "Epoch 41 - Training loss: 0.192\n",
            "On train set: Acc 0.938\n",
            "On test set: Acc 0.896\n",
            "Epoch 42 - Training loss: 0.181\n",
            "On train set: Acc 0.946\n",
            "On test set: Acc 0.888\n",
            "Epoch 43 - Training loss: 0.148\n",
            "On train set: Acc 0.944\n",
            "On test set: Acc 0.916\n",
            "Epoch 44 - Training loss: 0.182\n",
            "On train set: Acc 0.939\n",
            "On test set: Acc 0.875\n",
            "Epoch 45 - Training loss: 0.194\n",
            "On train set: Acc 0.940\n",
            "On test set: Acc 0.926\n",
            "Epoch 46 - Training loss: 0.171\n",
            "On train set: Acc 0.947\n",
            "On test set: Acc 0.849\n",
            "Epoch 47 - Training loss: 0.208\n",
            "On train set: Acc 0.925\n",
            "On test set: Acc 0.872\n",
            "Epoch 48 - Training loss: 0.214\n",
            "On train set: Acc 0.933\n",
            "On test set: Acc 0.902\n",
            "Epoch 49 - Training loss: 0.179\n",
            "On train set: Acc 0.938\n",
            "On test set: Acc 0.858\n",
            "Epoch 50 - Training loss: 0.187\n",
            "On train set: Acc 0.938\n",
            "On test set: Acc 0.882\n",
            "Epoch 51 - Training loss: 0.192\n",
            "On train set: Acc 0.937\n",
            "On test set: Acc 0.895\n",
            "Epoch 52 - Training loss: 0.158\n",
            "On train set: Acc 0.948\n",
            "On test set: Acc 0.909\n",
            "Epoch 53 - Training loss: 0.154\n",
            "On train set: Acc 0.945\n",
            "On test set: Acc 0.919\n",
            "Epoch 54 - Training loss: 0.142\n",
            "On train set: Acc 0.947\n",
            "On test set: Acc 0.909\n",
            "Epoch 55 - Training loss: 0.144\n",
            "On train set: Acc 0.953\n",
            "On test set: Acc 0.919\n",
            "Epoch 56 - Training loss: 0.159\n",
            "On train set: Acc 0.954\n",
            "On test set: Acc 0.906\n",
            "Epoch 57 - Training loss: 0.162\n",
            "On train set: Acc 0.949\n",
            "On test set: Acc 0.916\n",
            "Epoch 58 - Training loss: 0.127\n",
            "On train set: Acc 0.953\n",
            "On test set: Acc 0.926\n",
            "Epoch 59 - Training loss: 0.135\n",
            "On train set: Acc 0.955\n",
            "On test set: Acc 0.927\n",
            "Epoch 60 - Training loss: 0.120\n",
            "On train set: Acc 0.956\n",
            "On test set: Acc 0.916\n",
            "Epoch 61 - Training loss: 0.123\n",
            "On train set: Acc 0.961\n",
            "On test set: Acc 0.912\n",
            "Epoch 62 - Training loss: 0.113\n",
            "On train set: Acc 0.957\n",
            "On test set: Acc 0.930\n",
            "Epoch 63 - Training loss: 0.125\n",
            "On train set: Acc 0.954\n",
            "On test set: Acc 0.920\n",
            "Epoch 64 - Training loss: 0.114\n",
            "On train set: Acc 0.956\n",
            "On test set: Acc 0.919\n",
            "Epoch 65 - Training loss: 0.111\n",
            "On train set: Acc 0.956\n",
            "On test set: Acc 0.928\n",
            "Epoch 66 - Training loss: 0.141\n",
            "On train set: Acc 0.955\n",
            "On test set: Acc 0.921\n",
            "Epoch 67 - Training loss: 0.131\n",
            "On train set: Acc 0.957\n",
            "On test set: Acc 0.914\n",
            "Epoch 68 - Training loss: 0.124\n",
            "On train set: Acc 0.959\n",
            "On test set: Acc 0.926\n",
            "Epoch 69 - Training loss: 0.116\n",
            "On train set: Acc 0.962\n",
            "On test set: Acc 0.921\n",
            "Epoch 70 - Training loss: 0.123\n",
            "On train set: Acc 0.955\n",
            "On test set: Acc 0.944\n",
            "Epoch 71 - Training loss: 0.125\n",
            "On train set: Acc 0.959\n",
            "On test set: Acc 0.940\n",
            "Epoch 72 - Training loss: 0.134\n",
            "On train set: Acc 0.956\n",
            "On test set: Acc 0.907\n",
            "Epoch 73 - Training loss: 0.132\n",
            "On train set: Acc 0.954\n",
            "On test set: Acc 0.921\n",
            "Epoch 74 - Training loss: 0.110\n",
            "On train set: Acc 0.960\n",
            "On test set: Acc 0.921\n",
            "Epoch 75 - Training loss: 0.106\n",
            "On train set: Acc 0.961\n",
            "On test set: Acc 0.913\n",
            "Epoch 76 - Training loss: 0.115\n",
            "On train set: Acc 0.958\n",
            "On test set: Acc 0.909\n",
            "Epoch 77 - Training loss: 0.113\n",
            "On train set: Acc 0.962\n",
            "On test set: Acc 0.930\n",
            "Epoch 78 - Training loss: 0.109\n",
            "On train set: Acc 0.954\n",
            "On test set: Acc 0.910\n",
            "Epoch 79 - Training loss: 0.123\n",
            "On train set: Acc 0.956\n",
            "On test set: Acc 0.899\n",
            "Epoch 80 - Training loss: 0.130\n",
            "On train set: Acc 0.952\n",
            "On test set: Acc 0.920\n",
            "Best Train Acc: 0.9621318373071529\n",
            "Best Test Acc: 0.9438990182328191\n"
          ]
        }
      ],
      "source": [
        "model = TwoDCNN(num_class=num_class, dropout=0.01)\n",
        "train(model, train_loader, test_loader, eval_loader, 80, 0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Training loss: 1.976\n",
            "On train set: Acc 0.225\n",
            "On test set: Acc 0.114\n",
            "Epoch 2 - Training loss: 1.562\n",
            "On train set: Acc 0.444\n",
            "On test set: Acc 0.245\n",
            "Epoch 3 - Training loss: 1.331\n",
            "On train set: Acc 0.509\n",
            "On test set: Acc 0.278\n",
            "Epoch 4 - Training loss: 1.118\n",
            "On train set: Acc 0.560\n",
            "On test set: Acc 0.296\n",
            "Epoch 5 - Training loss: 0.970\n",
            "On train set: Acc 0.645\n",
            "On test set: Acc 0.317\n",
            "Epoch 6 - Training loss: 0.885\n",
            "On train set: Acc 0.702\n",
            "On test set: Acc 0.342\n",
            "Epoch 7 - Training loss: 0.772\n",
            "On train set: Acc 0.725\n",
            "On test set: Acc 0.372\n",
            "Epoch 8 - Training loss: 0.744\n",
            "On train set: Acc 0.749\n",
            "On test set: Acc 0.412\n",
            "Epoch 9 - Training loss: 0.689\n",
            "On train set: Acc 0.758\n",
            "On test set: Acc 0.429\n",
            "Epoch 10 - Training loss: 0.667\n",
            "On train set: Acc 0.769\n",
            "On test set: Acc 0.474\n",
            "Epoch 11 - Training loss: 0.619\n",
            "On train set: Acc 0.757\n",
            "On test set: Acc 0.461\n",
            "Epoch 12 - Training loss: 0.582\n",
            "On train set: Acc 0.795\n",
            "On test set: Acc 0.492\n",
            "Epoch 13 - Training loss: 0.581\n",
            "On train set: Acc 0.777\n",
            "On test set: Acc 0.432\n",
            "Epoch 14 - Training loss: 0.603\n",
            "On train set: Acc 0.765\n",
            "On test set: Acc 0.449\n",
            "Epoch 15 - Training loss: 0.625\n",
            "On train set: Acc 0.761\n",
            "On test set: Acc 0.453\n",
            "Epoch 16 - Training loss: 0.584\n",
            "On train set: Acc 0.797\n",
            "On test set: Acc 0.520\n",
            "Epoch 17 - Training loss: 0.592\n",
            "On train set: Acc 0.785\n",
            "On test set: Acc 0.662\n",
            "Epoch 18 - Training loss: 0.573\n",
            "On train set: Acc 0.798\n",
            "On test set: Acc 0.783\n",
            "Epoch 19 - Training loss: 0.497\n",
            "On train set: Acc 0.804\n",
            "On test set: Acc 0.767\n",
            "Epoch 20 - Training loss: 0.534\n",
            "On train set: Acc 0.797\n",
            "On test set: Acc 0.725\n",
            "Epoch 21 - Training loss: 0.537\n",
            "On train set: Acc 0.802\n",
            "On test set: Acc 0.813\n",
            "Epoch 22 - Training loss: 0.491\n",
            "On train set: Acc 0.815\n",
            "On test set: Acc 0.798\n",
            "Epoch 23 - Training loss: 0.471\n",
            "On train set: Acc 0.837\n",
            "On test set: Acc 0.806\n",
            "Epoch 24 - Training loss: 0.446\n",
            "On train set: Acc 0.832\n",
            "On test set: Acc 0.812\n",
            "Epoch 25 - Training loss: 0.445\n",
            "On train set: Acc 0.836\n",
            "On test set: Acc 0.748\n",
            "Epoch 26 - Training loss: 0.425\n",
            "On train set: Acc 0.830\n",
            "On test set: Acc 0.771\n",
            "Epoch 27 - Training loss: 0.419\n",
            "On train set: Acc 0.836\n",
            "On test set: Acc 0.769\n",
            "Epoch 28 - Training loss: 0.371\n",
            "On train set: Acc 0.857\n",
            "On test set: Acc 0.774\n",
            "Epoch 29 - Training loss: 0.388\n",
            "On train set: Acc 0.857\n",
            "On test set: Acc 0.787\n",
            "Epoch 30 - Training loss: 0.382\n",
            "On train set: Acc 0.857\n",
            "On test set: Acc 0.850\n",
            "Epoch 31 - Training loss: 0.384\n",
            "On train set: Acc 0.857\n",
            "On test set: Acc 0.849\n",
            "Epoch 32 - Training loss: 0.373\n",
            "On train set: Acc 0.865\n",
            "On test set: Acc 0.877\n",
            "Epoch 33 - Training loss: 0.369\n",
            "On train set: Acc 0.865\n",
            "On test set: Acc 0.770\n",
            "Epoch 34 - Training loss: 0.376\n",
            "On train set: Acc 0.869\n",
            "On test set: Acc 0.753\n",
            "Epoch 35 - Training loss: 0.422\n",
            "On train set: Acc 0.846\n",
            "On test set: Acc 0.778\n",
            "Epoch 36 - Training loss: 0.339\n",
            "On train set: Acc 0.881\n",
            "On test set: Acc 0.822\n",
            "Epoch 37 - Training loss: 0.340\n",
            "On train set: Acc 0.885\n",
            "On test set: Acc 0.767\n",
            "Epoch 38 - Training loss: 0.325\n",
            "On train set: Acc 0.890\n",
            "On test set: Acc 0.777\n",
            "Epoch 39 - Training loss: 0.397\n",
            "On train set: Acc 0.855\n",
            "On test set: Acc 0.696\n",
            "Epoch 40 - Training loss: 0.381\n",
            "On train set: Acc 0.863\n",
            "On test set: Acc 0.835\n",
            "Epoch 41 - Training loss: 0.367\n",
            "On train set: Acc 0.873\n",
            "On test set: Acc 0.806\n",
            "Epoch 42 - Training loss: 0.330\n",
            "On train set: Acc 0.865\n",
            "On test set: Acc 0.771\n",
            "Epoch 43 - Training loss: 0.334\n",
            "On train set: Acc 0.879\n",
            "On test set: Acc 0.854\n",
            "Epoch 44 - Training loss: 0.323\n",
            "On train set: Acc 0.884\n",
            "On test set: Acc 0.857\n",
            "Epoch 45 - Training loss: 0.304\n",
            "On train set: Acc 0.886\n",
            "On test set: Acc 0.827\n",
            "Epoch 46 - Training loss: 0.324\n",
            "On train set: Acc 0.886\n",
            "On test set: Acc 0.835\n",
            "Epoch 47 - Training loss: 0.335\n",
            "On train set: Acc 0.883\n",
            "On test set: Acc 0.763\n",
            "Epoch 48 - Training loss: 0.336\n",
            "On train set: Acc 0.879\n",
            "On test set: Acc 0.857\n",
            "Epoch 49 - Training loss: 0.305\n",
            "On train set: Acc 0.889\n",
            "On test set: Acc 0.787\n",
            "Epoch 50 - Training loss: 0.294\n",
            "On train set: Acc 0.894\n",
            "On test set: Acc 0.893\n",
            "Epoch 51 - Training loss: 0.261\n",
            "On train set: Acc 0.908\n",
            "On test set: Acc 0.849\n",
            "Epoch 52 - Training loss: 0.275\n",
            "On train set: Acc 0.912\n",
            "On test set: Acc 0.832\n",
            "Epoch 53 - Training loss: 0.256\n",
            "On train set: Acc 0.910\n",
            "On test set: Acc 0.826\n",
            "Epoch 54 - Training loss: 0.249\n",
            "On train set: Acc 0.913\n",
            "On test set: Acc 0.886\n",
            "Epoch 55 - Training loss: 0.225\n",
            "On train set: Acc 0.928\n",
            "On test set: Acc 0.879\n",
            "Epoch 56 - Training loss: 0.216\n",
            "On train set: Acc 0.923\n",
            "On test set: Acc 0.919\n",
            "Epoch 57 - Training loss: 0.198\n",
            "On train set: Acc 0.933\n",
            "On test set: Acc 0.884\n",
            "Epoch 58 - Training loss: 0.187\n",
            "On train set: Acc 0.925\n",
            "On test set: Acc 0.900\n",
            "Epoch 59 - Training loss: 0.196\n",
            "On train set: Acc 0.932\n",
            "On test set: Acc 0.893\n",
            "Epoch 60 - Training loss: 0.197\n",
            "On train set: Acc 0.922\n",
            "On test set: Acc 0.774\n",
            "Epoch 61 - Training loss: 0.224\n",
            "On train set: Acc 0.920\n",
            "On test set: Acc 0.787\n",
            "Epoch 62 - Training loss: 0.213\n",
            "On train set: Acc 0.920\n",
            "On test set: Acc 0.748\n",
            "Epoch 63 - Training loss: 0.240\n",
            "On train set: Acc 0.924\n",
            "On test set: Acc 0.835\n",
            "Epoch 64 - Training loss: 0.197\n",
            "On train set: Acc 0.929\n",
            "On test set: Acc 0.882\n",
            "Epoch 65 - Training loss: 0.219\n",
            "On train set: Acc 0.927\n",
            "On test set: Acc 0.889\n",
            "Epoch 66 - Training loss: 0.227\n",
            "On train set: Acc 0.923\n",
            "On test set: Acc 0.881\n",
            "Epoch 67 - Training loss: 0.216\n",
            "On train set: Acc 0.915\n",
            "On test set: Acc 0.900\n",
            "Epoch 68 - Training loss: 0.211\n",
            "On train set: Acc 0.924\n",
            "On test set: Acc 0.907\n",
            "Epoch 69 - Training loss: 0.207\n",
            "On train set: Acc 0.931\n",
            "On test set: Acc 0.860\n",
            "Epoch 70 - Training loss: 0.182\n",
            "On train set: Acc 0.934\n",
            "On test set: Acc 0.916\n",
            "Epoch 71 - Training loss: 0.203\n",
            "On train set: Acc 0.931\n",
            "On test set: Acc 0.921\n",
            "Epoch 72 - Training loss: 0.192\n",
            "On train set: Acc 0.930\n",
            "On test set: Acc 0.833\n",
            "Epoch 73 - Training loss: 0.186\n",
            "On train set: Acc 0.937\n",
            "On test set: Acc 0.917\n",
            "Epoch 74 - Training loss: 0.168\n",
            "On train set: Acc 0.934\n",
            "On test set: Acc 0.909\n",
            "Epoch 75 - Training loss: 0.172\n",
            "On train set: Acc 0.933\n",
            "On test set: Acc 0.889\n",
            "Epoch 76 - Training loss: 0.160\n",
            "On train set: Acc 0.936\n",
            "On test set: Acc 0.858\n",
            "Epoch 77 - Training loss: 0.178\n",
            "On train set: Acc 0.948\n",
            "On test set: Acc 0.870\n",
            "Epoch 78 - Training loss: 0.185\n",
            "On train set: Acc 0.938\n",
            "On test set: Acc 0.917\n",
            "Epoch 79 - Training loss: 0.168\n",
            "On train set: Acc 0.943\n",
            "On test set: Acc 0.886\n",
            "Epoch 80 - Training loss: 0.173\n",
            "On train set: Acc 0.939\n",
            "On test set: Acc 0.889\n",
            "Epoch 81 - Training loss: 0.153\n",
            "On train set: Acc 0.947\n",
            "On test set: Acc 0.919\n",
            "Epoch 82 - Training loss: 0.170\n",
            "On train set: Acc 0.946\n",
            "On test set: Acc 0.895\n",
            "Epoch 83 - Training loss: 0.149\n",
            "On train set: Acc 0.944\n",
            "On test set: Acc 0.867\n",
            "Epoch 84 - Training loss: 0.158\n",
            "On train set: Acc 0.946\n",
            "On test set: Acc 0.896\n",
            "Epoch 85 - Training loss: 0.147\n",
            "On train set: Acc 0.941\n",
            "On test set: Acc 0.910\n",
            "Epoch 86 - Training loss: 0.144\n",
            "On train set: Acc 0.952\n",
            "On test set: Acc 0.843\n",
            "Epoch 87 - Training loss: 0.170\n",
            "On train set: Acc 0.939\n",
            "On test set: Acc 0.872\n",
            "Epoch 88 - Training loss: 0.225\n",
            "On train set: Acc 0.924\n",
            "On test set: Acc 0.891\n",
            "Epoch 89 - Training loss: 0.194\n",
            "On train set: Acc 0.929\n",
            "On test set: Acc 0.885\n",
            "Epoch 90 - Training loss: 0.212\n",
            "On train set: Acc 0.928\n",
            "On test set: Acc 0.823\n",
            "Epoch 91 - Training loss: 0.222\n",
            "On train set: Acc 0.917\n",
            "On test set: Acc 0.875\n",
            "Epoch 92 - Training loss: 0.214\n",
            "On train set: Acc 0.928\n",
            "On test set: Acc 0.877\n",
            "Epoch 93 - Training loss: 0.171\n",
            "On train set: Acc 0.930\n",
            "On test set: Acc 0.870\n",
            "Epoch 94 - Training loss: 0.152\n",
            "On train set: Acc 0.941\n",
            "On test set: Acc 0.927\n",
            "Epoch 95 - Training loss: 0.146\n",
            "On train set: Acc 0.952\n",
            "On test set: Acc 0.907\n",
            "Epoch 96 - Training loss: 0.174\n",
            "On train set: Acc 0.935\n",
            "On test set: Acc 0.930\n",
            "Epoch 97 - Training loss: 0.148\n",
            "On train set: Acc 0.952\n",
            "On test set: Acc 0.913\n",
            "Epoch 98 - Training loss: 0.136\n",
            "On train set: Acc 0.946\n",
            "On test set: Acc 0.913\n",
            "Epoch 99 - Training loss: 0.140\n",
            "On train set: Acc 0.947\n",
            "On test set: Acc 0.927\n",
            "Epoch 100 - Training loss: 0.126\n",
            "On train set: Acc 0.954\n",
            "On test set: Acc 0.903\n",
            "Epoch 101 - Training loss: 0.142\n",
            "On train set: Acc 0.949\n",
            "On test set: Acc 0.930\n",
            "Epoch 102 - Training loss: 0.132\n",
            "On train set: Acc 0.955\n",
            "On test set: Acc 0.920\n",
            "Epoch 103 - Training loss: 0.125\n",
            "On train set: Acc 0.948\n",
            "On test set: Acc 0.920\n",
            "Epoch 104 - Training loss: 0.120\n",
            "On train set: Acc 0.956\n",
            "On test set: Acc 0.893\n",
            "Epoch 105 - Training loss: 0.124\n",
            "On train set: Acc 0.963\n",
            "On test set: Acc 0.927\n",
            "Epoch 106 - Training loss: 0.131\n",
            "On train set: Acc 0.955\n",
            "On test set: Acc 0.924\n",
            "Epoch 107 - Training loss: 0.123\n",
            "On train set: Acc 0.954\n",
            "On test set: Acc 0.913\n",
            "Epoch 108 - Training loss: 0.124\n",
            "On train set: Acc 0.957\n",
            "On test set: Acc 0.917\n",
            "Epoch 109 - Training loss: 0.119\n",
            "On train set: Acc 0.953\n",
            "On test set: Acc 0.931\n",
            "Epoch 110 - Training loss: 0.117\n",
            "On train set: Acc 0.951\n",
            "On test set: Acc 0.933\n",
            "Epoch 111 - Training loss: 0.107\n",
            "On train set: Acc 0.963\n",
            "On test set: Acc 0.934\n",
            "Epoch 112 - Training loss: 0.108\n",
            "On train set: Acc 0.961\n",
            "On test set: Acc 0.917\n",
            "Epoch 113 - Training loss: 0.108\n",
            "On train set: Acc 0.961\n",
            "On test set: Acc 0.927\n",
            "Epoch 114 - Training loss: 0.102\n",
            "On train set: Acc 0.959\n",
            "On test set: Acc 0.931\n",
            "Epoch 115 - Training loss: 0.100\n",
            "On train set: Acc 0.957\n",
            "On test set: Acc 0.909\n",
            "Epoch 116 - Training loss: 0.110\n",
            "On train set: Acc 0.956\n",
            "On test set: Acc 0.930\n",
            "Epoch 117 - Training loss: 0.097\n",
            "On train set: Acc 0.961\n",
            "On test set: Acc 0.927\n",
            "Epoch 118 - Training loss: 0.114\n",
            "On train set: Acc 0.961\n",
            "On test set: Acc 0.935\n",
            "Epoch 119 - Training loss: 0.113\n",
            "On train set: Acc 0.952\n",
            "On test set: Acc 0.919\n",
            "Epoch 120 - Training loss: 0.101\n",
            "On train set: Acc 0.961\n",
            "On test set: Acc 0.900\n",
            "Best Train Acc: 0.9628330995792427\n",
            "Best Test Acc: 0.9354838709677419\n"
          ]
        }
      ],
      "source": [
        "model = TFTwoDCNN(num_class=num_class, dropout=0.01)\n",
        "train(model, train_loader, test_loader, eval_loader, 120, 0.05)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "sitting_posture",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
